# ğŸ•µï¸â€â™‚ï¸ Incremental Web Scraper for Leprosy/Hansen's Disease Research

This project is an **incremental web scraper** designed to collect, store, and analyze online information related to **Leprosy (Hansen's Disease)**.  
It prevents duplicate data by maintaining a **local history** of processed URLs and provides **automatic visualizations** for quick insights.

---

## âœ¨ Features

- ğŸ” **Incremental Search**: Collects only new URLs that were not processed before.  
- ğŸŒ **Multi-source scraping**:
  - Google (smart queries with date filters)  
  - PubMed (academic articles)  
  - Specialized sources (WHO, CDC, NLR, Leprosy Mission, etc.)  
- ğŸ“Š **Automatic analysis and visualizations**:
  - Status of scraped URLs (success/errors)  
  - Sources distribution (Google, PubMed, Journals, etc.)  
  - Database growth over time  
  - Most frequent domains  
  - Abstract lengths distribution  
  - Automatic topic detection (AI, ML, mHealth, Telemedicine, etc.)  
- ğŸ’¾ **History management**: Stores processed URLs to avoid duplication in future runs.  
- âš¡ **Thread support**: Option to scrape multiple URLs in parallel.  

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ main.py                  # Entry point of the project
â”œâ”€â”€ functions.py             # Scraper logic, incremental execution, and visualizations
â”œâ”€â”€ web_scraper_historico.py # History manager and web scraping strategies
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md                # Project documentation
```

---

## âš™ï¸ Installation

Clone the repository:

```bash
git clone https://github.com/your-username/leprosy-webscraper.git
cd leprosy-webscraper
```

Create a virtual environment (recommended):

```bash
python -m venv venv
source venv/bin/activate   # Linux / Mac
venv\Scripts\activate      # Windows
```

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

Run the incremental scraper:

```bash
python main.py
```

By default, it runs the single-thread incremental scraper.  
To enable **multi-threading** (faster execution), edit `main.py`:

```python
# result = executar_scraper_incremental(max_urls=120)
result = executar_scraper_incremental_threads(max_urls=120)
```

---

## ğŸ“Š Output

- All scraped results are saved as a **CSV file** with timestamp:
  ```
  leprosy_incremental_YYYYMMDD_HHMMSS.csv
  ```
- The scraper also updates a **history file**:
  ```
  historico_leprosy_colab.json
  ```

